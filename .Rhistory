V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.0007&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
fivenum(page.rank(graph4)$vector)  ## .. 0.0006 0.004
graph4 <- delete.vertices(graph,V(graph)[V(graph)$language!="English"])  ## 5260 nodes now
graph4 <- delete.vertices(graph4,V(graph4)[authority.score(graph4)$vector<0.5&
V(graph4)$publication_year<2012&
V(graph4)$times_cited<500&
V(graph4)$keyword_count==0])  ## 2878 nodes now
graph4 <- delete.vertices(graph4,V(graph4)[page.rank(graph4)$vector<0.001&
V(graph4)$times_cited<500&
V(graph4)$keyword_count==0]) # 1999 nodes now
graph4 <- delete.vertices(graph4,V(graph4)[V(graph4)$times_cited<2&
V(graph4)$publication_year<2013]) ## 1839 nodes now
fivenum(page.rank(graph4)$vector)  ## .. 0.0006 0.004
table(V(graph4)$keyword_count)   ## 0:104,1:1091,2:460,3:146,4:28,5:8,6:2
length(V(graph4)[page.rank(graph4)$vector<0.0007&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.001&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.001&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.9&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.002&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.003&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.003&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<3]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.0035&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.0039&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.004&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.003&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.001&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
length(V(graph4)[page.rank(graph4)$vector<0.003&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 810
graph4 <- delete.vertices(graph4,V(graph4)[page.rank(graph4)$vector<0.003&
authority.score(graph4)$vector<0.5&
V(graph4)$times_cited<500&
V(graph4)$keyword_count<2]) # 1029 nodes now
vcount(graph4)  # 540
ecount(graph4)  # 1662
graph4 <- delete.vertices(graph,V(graph)[V(graph)$language!="English"])  ## 5260 nodes now
length(V(graph4)[authority.score(graph4)$vector<0.5&
V(graph4)$publication_year<2012&
V(graph4)$times_cited<500&
V(graph4)$keyword_count==0])   ## 2382
length(V(graph4)[authority.score(graph4)$vector<0.9&
V(graph4)$publication_year<2012&
V(graph4)$times_cited<500&
V(graph4)$keyword_count==0])   ## 2382
graph4 <- delete.vertices(graph4,V(graph4)[authority.score(graph4)$vector<0.5&
V(graph4)$publication_year<2012&
V(graph4)$times_cited<500&
V(graph4)$keyword_count==0])  ## 2878 nodes now
fivenum(page.rank(graph4)$vector)  ## .. 0.0004 0.004
length(V(graph4)[page.rank(graph4)$vector<0.001&
V(graph4)$times_cited<500&
V(graph4)$keyword_count==0]) # 879
length(V(graph4)[page.rank(graph4)$vector<0.004&
V(graph4)$times_cited<500&
V(graph4)$keyword_count==0]) # 879
length(V(graph4)[page.rank(graph4)$vector<0.003&
V(graph4)$times_cited<500&
V(graph4)$keyword_count==0]) # 879
graph4 <- delete.vertices(graph4,V(graph4)[page.rank(graph4)$vector<0.003&
V(graph4)$times_cited<500&
V(graph4)$keyword_count==0]) # 1999 nodes now
vcount(graph4)  # 760
fivenum(V(graph4)$times_cited)  ## .. 1 5 15983
length(V(graph4)[V(graph4)$times_cited<2&
V(graph4)$publication_year<2013])   ## 160
graph4 <- delete.vertices(graph4,V(graph4)[V(graph4)$times_cited<2&
V(graph4)$publication_year<2013]) ## 1839 nodes now
vcount(graph4)  # 760
fivenum(page.rank(graph4)$vector)  ## .. 0.0006 0.004
table(V(graph4)$keyword_count)   ## 0:104,1:1091,2:460,3:146,4:28,5:8,6:2
length(V(graph4)[page.rank(graph4)$vector<0.003&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<2]) # 1079
length(V(graph4)[page.rank(graph4)$vector<0.003&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<3]) # 1079
length(V(graph4)[page.rank(graph4)$vector<0.001&
V(graph4)$times_cited<500&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<3]) # 1079
graph4 <- delete.vertices(graph4,V(graph4)[page.rank(graph4)$vector<0.001&
authority.score(graph4)$vector<0.5&
V(graph4)$times_cited<500&
V(graph4)$keyword_count<3]) # 760 nodes now
vcount(graph4)  # 760
ecount(graph4)  # 2771
table(V(graph4)$keyword_count)   ## 0:69,1:1091,2:460,3:146,4:28,5:8,6:2
V(graph4)$authority_score <- authority.score(graph4)$vector
V(graph4)$page_rank <- page.rank(graph4)$vector
V(graph4)$hub_score <- hub.score(graph4)$vector
list.vertex.attributes(graph4)
graph4 <- remove.vertex.attribute(graph4,"abstract_text")
graph4 <- remove.vertex.attribute(graph4,"article_number")
graph4 <- remove.vertex.attribute(graph4,"beginning_page")
graph4 <- remove.vertex.attribute(graph4,"ending_page")
graph4 <- remove.vertex.attribute(graph4,"cited_reference_count")
graph4 <- remove.vertex.attribute(graph4,"cited_year")
graph4 <- remove.vertex.attribute(graph4,"document_volume")
graph4 <- remove.vertex.attribute(graph4,"funding_agency_and_grant_number")
graph4 <- remove.vertex.attribute(graph4,"funding_text")
graph4 <- remove.vertex.attribute(graph4,"isi_document_delivery_number")
graph4 <- remove.vertex.attribute(graph4,"issue")
graph4 <- remove.vertex.attribute(graph4,"page_count")
graph4 <- remove.vertex.attribute(graph4,"part_number")
graph4 <- remove.vertex.attribute(graph4,"special_issue")
graph4 <- remove.vertex.attribute(graph4,"supplement")
graph4 <- remove.vertex.attribute(graph4,"_x_nwb_id")
graph4 <- remove.vertex.attribute(graph4,"id")
graph4 <- remove.vertex.attribute(graph4,"isbn")
graph4 <- remove.vertex.attribute(graph4,"digital_object_identifier")
graph4 <- remove.vertex.attribute(graph4,"nwbWeightedPagerank")
graph4 <- remove.vertex.attribute(graph4,"language")
list.vertex.attributes(graph4)
table(V(graph4)$document_type)
length(V(graph4)[page.rank(graph4)$vector<0.001&
V(graph4)$document_type=="Review"&
authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<3]) # 1373
length(V(graph4)[page.rank(graph4)$vector<0.001&
V(graph4)$document_type=="Review"&
//    authority.score(graph4)$vector<0.5&
length(V(graph4)[page.rank(graph4)$vector<0.001&
V(graph4)$document_type=="Review"&
#    authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<3]) # 1373
length(V(graph4)[page.rank(graph4)$vector<0.005&
V(graph4)$document_type=="Review"&
#    authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<3]) # 1373
length(V(graph4)[page.rank(graph4)$vector<0.005&
V(graph4)$document_type=="Review"&
#    authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<1]) # 1373
length(V(graph4)[page.rank(graph4)$vector<0.005&
V(graph4)$document_type!="Article"&
#    authority.score(graph4)$vector<0.5&
V(graph4)$keyword_count<1]) # 19
vcount(graph4)  # 431
hist(table(V(graph4)$keyword_count))
plot(table(V(graph4)$keyword_count))
barplot(table(V(graph4)$keyword_count))
library("ggplot2", lib.loc="~/R/win-library/3.1")
barplot(V(graph4)$keyword_count)
ggplot(table(V(graph4)$keyword_count))
ggplot(V(graph4)$keyword_count, aes(x=keyword, y=count))+
geom_bar(stat="identity")
ggplot(V(graph4)$document_type, aes(x=keyword, y=count))+
geom_bar(stat="identity")
barplot(V(graph4)$document_type)
barplot(table(V(graph4)$document_type))
barplot(table(V(graph4)$document_type),names.arg=gsub("\\s","\n", V(graph4)$document_type))
plot(hist(V(graph4)$document_type),names.arg=gsub("\\s","\n", V(graph4)$document_type))
plot(hist(V(graph4)$keyword_count),names.arg=gsub("\\s","\n", V(graph4)$document_type))
plot(hist(V(graph4)$keyword_count),)
plot(hist(V(graph4)$keyword_count))
plot(hist(V(graph4)$keyword_count), labels=TRUE)
histinfo(V(graph4)$keyword_count)
histInfo(V(graph4)$keyword_count)
table(V(graph4)$document_type)  ## review:60, article:338
x<-c("article","article_book_chapter","article_proceedings_paper","editorial_material",
"letter","note","proceedings_paper","review","review_book_chapter")
y<-c(338,6,10,8,1,1,4,60,3)
barplot(x,y, labels=TRUE)
plot(x,y, labels=TRUE)
plot(x,y)
plot(y~x)
qplot(V(graph4)$document_type)
qplot(V(graph4)$document_type,binwidth=0.5)
qplot(V(graph4)$document_type,binwidth=0.9)
qplot(V(graph4)$document_type,binwidth=1)
qplot(V(graph4)$document_type,binwidth=2)
qplot(V(graph4)$document_type,geom_text(aes(label = Frequency, y = pos), size = 3))
qplot(V(graph4)$document_type,geom_text(aes(label = Frequency), size = 3))
qplot(V(graph4)$document_type,geom_text(aes(label = Frequency)))
qplot(V(graph4)$document_type,color=blue)
qplot(V(graph4)$document_type,color="blue")
qplot(V(graph4)$document_type, color="blue")
qplot(V(graph4)$document_type, color="red")
qplot(V(graph4)$document_type)
qplot(table(V(graph4)$keyword_count))
ggplot(as.data.frame(table(V(graph4)$keyword_count), aes(x=gender, y = Freq, fill=fraud)) + geom_bar(stat="identity")
ggplot(as.data.frame(table(V(graph4)$keyword_count), aes(x=gender, y = Freq, fill=fraud)) + geom_bar(stat="identity"))
ggplot(as.data.frame(table(V(graph4)$keyword_count), aes(x=gender, y = Freq, fill=fraud)) + geom_bar(stat="identity"))
ggplot(as.data.frame(table(V(graph4)$keyword_count), geom_bar(stat="identity"))
ggplot(as.data.frame(table(V(graph4)$keyword_count), geom_bar(stat="identity")))
ggplot(as.data.frame(table(V(graph4)$keyword_count), geom_bar(stat="identity")))
table(V(graph4)$keyword_count)   ## 0:69,1:128,2:50,3:146,4:28,5:8,6:2
x<-c(0,1,2,3,4,5,6)
y<-c(69,128,50,146,28,8,2)
qplot(x,y)
qplot(x,y,geom="histogram")
qplot(x,y,geom="line")
qplot(x,y,geom=c("line","smooth"))
qplot(x,y,geom=c("point","smooth"))
qplot(x,y,geom="line")
qplot(x,y,geom="bar")
qplot(x,y)
barplot(x,y)
barplot(x,y,labels=TRUE)
qplot(x,y,labels=TRUE)
qplot(x,y)
qplot(x,y,geom="line")
barplot(table(x,y))
qplot(x,y,geom="line")
qplot(x,y,geom="line",xlab="keyword counts")
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles")
geom_bar(as.data.frame(x,y))
geom_bar(as.data.frame(table(x,y))
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles")
qplot(x,y,geom="bar",xlab="keyword counts", ylab="number of articles")
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",stat="identity")
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles")
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",binwidth=0.5)
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",binwidth=1)
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",binwidth=2)
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",binwidth=0.2)
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",scale_x_continuous(limits=c(0, 7)))
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",xlim(0, 7))
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",xlim(0, 10))
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",
scale_x_continuous(breaks=1:10))
scale_x_continuous(breaks=0:7))
qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles",
scale_x_continuous(breaks=0:7))
q<-qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles")
q+scale_x_continuous(breaks=0:7)
q
q<-qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles")
q+scale_x_continuous(breaks=0:7)
q+scale_y_continuous(breaks=0:150)
q<-qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles")
q+scale_x_continuous(breaks=0:7)
q+scale_y_continuous(breaks=0:6)
q+scale_y_continuous(breaks=0:150)
q<-qplot(x,y,geom="line",xlab="keyword counts", ylab="number of articles")
q+scale_x_continuous(breaks=0:7)
write.graph(graph4,file="C:\\Users\\nibu\\Google Drive\\data science\\Katy Borner\\project\\cleaned files\\document_citation_network_scaled_down_431.graphml","graphml")
sum(degree(graph4)==0)
graph4 <- delete.vertices(graph4, which(degree(graph4) < 1)-1)
vcount(graph4)  # 431
ecount(graph4)  # 1360
table(V(graph4)$keyword_count)   ## 0:69,1:128,2:50,3:146,4:28,5:8,6:2
Authors <- read.csv("C:/Users/nibu/Google Drive/data science/Katy Borner/project/cleaned files/Authors.csv")
View(Authors)
authors<-Authors[c(2,4,8),]
View(authors)
authors<-Authors[,c(2,4,8)]
View(authors)
authorsPaper<-authors[order(authors$PAPERS_AUTHORED_IN_DATASET), ]
View(authorsPaper)
authorsPaper<-authors[order(authors$PAPERS_AUTHORED_IN_DATASET, desc=T), ]
authorsPaper<-authors[order(authors$PAPERS_AUTHORED_IN_DATASET, decreasing=T), ]
View(authorsPaper)
authorsPaper<-head(authorsPaper,40)
View(authorsPaper)
View(Authors)
authors<-Authors[,c(2,3,8)]
authorsPaper<-authors[order(authors$PAPERS_AUTHORED_IN_DATASET, decreasing=T), ]
authorsPaper<-head(authorsPaper,40)
View(authors)
authorsCitation<-authors[order(authors$GLOBAL_CITATION_COUNT, decreasing=T), ]
authorsCitation<-head(authorsCitation,40)
View(authorsCitation)
View(authorsPaper)
write.csv(authorsPaper,"C:\\Users\\nibu\\Google Drive\\data science\\Katy Borner\\project\\top-40_authorsByPaper.csv")
write.csv(authorsCitation,"C:\\Users\\nibu\\Google Drive\\data science\\Katy Borner\\project\\top-40_authorsByCitation.csv")
graphjson<-read.graph("C:\\Users\\nibu\\Google Drive\\data science\\Katy Borner\\project\\data.json",format="json")
library("igraph", lib.loc="~/R/win-library/3.1")
graphjson<-read.graph("C:\\Users\\nibu\\Google Drive\\data science\\Katy Borner\\project\\data.json",format="json")
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR() # this will start the updating process of your R installation.  It will check for newer versions, and if one is available, will guide you through the decisions you'd need to make.
updateR()
version
version
.libPath()
.libPaths()
.libPaths("C:\Users\nibu\Google Drive\R\win-library")
.libPaths("C:/Users/nibu/Google Drive/R/win-library")
.libPaths()
.libPaths()
.libPaths("C:/Users/nibu/Google Drive/R/win-library")
.libPaths()
.libPaths()
subscriber_data_from_standard_register <- read.csv("C:/Users/nibu/Google Drive/R/subscriber_data_from_standard_register.csv", stringsAsFactors=FALSE)
View(subscriber_data_from_standard_register)
subs <- subscriber_data_from_standard_register
subs$id <- as.numeric(rownames(subs))
View(subs)
dups <- data.frame(table(subs$username))
View(dups)
dups <- subs[subs$username %in% dups$Var1[dups$Freq > 1],]
dups <- data.frame(table(subs$username))
dups_subs <- subs[subs$username %in% dups$Var1[dups$Freq > 1],]
View(dups_subs)
dupsubs <- split(dups_subs, dups_subs$username)
dupsubs[[1]]
View(dups_subs)
dupsubs[[2]]
seq(dupsubs)
d <- dupsubs[[1]]
View(d)
table(d$subscriber_group)
dg <- data.frametable(d$subscriber_group)
dg <- data.frame(table(d$subscriber_group))
View(dg)
nrow(dg)
issue <- c()
for (i in seq(dupsubs)){
d <- dupsubs[[i]]
dg <- data.frame(table(d$subscriber_group))
if(nrow(dg)>1){issue <<- c(issue, d$username)}
}
issue <- c()
for (i in seq(dupsubs)){
d <- dupsubs[[i]]
dg <- data.frame(table(d$subscriber_group))
if(nrow(dg)>1){issue <<- c(issue, d$username[1])}
}
names(check) <- names(subs)
check <- data.frame()
names(check) <- names(subs)
check <- data.frame(names(subs))
View(check)
check <- subs[FALSE,]
View(check)
for (i in seq(dupsubs)){
d <- dupsubs[[i]]
dg <- data.frame(table(d$subscriber_group))
if(nrow(dg)>1){check <<- rbind(check, d)}
}
View(check)
acdups <- data.frame(table(check$account_number))
acdups_subs <- check[check$account_number %in% acdups$Var1[acdups$Freq > 1],]
View(acdups_subs)
repeated <- subs[FALSE,]  # repeated enrollments on one acc by same user
check <- subs[FALSE,]     # different acc and groups by same user
for (i in seq(dupsubs)){
d <- dupsubs[[i]]
dg <- data.frame(table(d$subscriber_group))
ac <- data.frame(table(d$account_number))
if(nrow(dg)>1){
if(nrow(ac)==1){repeated <<- rbind(repeated, d)}
else {check <<- rbind(check, d)}
}
}
View(repeated)
length(unique(repeated$account_number))
length(unique(check$account_number))
length(unique(repeated$username))  # 128 subscribers
length(unique(check$username))  # 4534 subscribers
length(unique(subs$username))
length(unique(subs$account_number))
length(unique(subs$email))
data <- c(814,362,764,809,223,1066,331,890,699,817,440,510,990)
data <- c(814,362,764,809,223,1066,331,890,699,817,440,510,990)
fivenum(data)
mean(data)
sd(data)
var(data)
d1 <- c(16.25,12.91,14.58)
mean(d1)
library(foreign)
R.version()
R.version.string
setwd("C:\Users\nibu\Google Drive\data science\Sriram\project\ESS_income_prediction")
setwd('C:\Users\nibu\Google Drive\data science\Sriram\project\ESS_income_prediction')
setwd('C:/Users/nibu/Google Drive/data science/Sriram/project/ESS_income_prediction')
install.packages("caret", dependencies = c("Depends", "Suggests"))
require(caret)
install.packages("ModelMetrics")
require(caret)
library("haven")
install.packages("haven")
require(haven)
install.packages("assertthat")
require(haven)
data_raw <- read_spss("ESS6MDWe02.2_F1.sav")  # original raw data
data <- data_raw         # working copy of the data
dim(data)                # 1847 obs. of 32 variables
data$ESS6_id  <- NULL
data$cntry    <- NULL
data$ESS6_reg <- NULL
data$NUTS1    <- NULL
data$NUTS2    <- NULL
data$NUTS3    <- NULL
data$dweight  <- NULL
data$pspwght  <- NULL
data$pweight  <- NULL
dim(data)                # 1847 obs. of 23 variables
names(data)
names(data)[names(data) == 'agea'] <- 'age'
names(data)[names(data) == 'hinctnta'] <- 'income'
summary(data)            # all columns have NA values; age needs normalization
data_withNA <- data
data <- data[complete.cases(data),]  # 1613 obs.
# include only people of age 30 years or more
# --------------------------------------------------------
data <- data[(data$age >= 30),]
nrow(data)               # 1300 obs.
table(data$age)          # only few people above 90; therefore can cluster 80s and 90s into one
data$age <- replace(data$age, data$age < 40, 1)
data$age <- replace(data$age, data$age >= 40 & data$age < 50, 2)
data$age <- replace(data$age, data$age >= 50 & data$age < 60, 3)
data$age <- replace(data$age, data$age >= 60 & data$age < 70, 4)
data$age <- replace(data$age, data$age >= 70 & data$age < 80, 5)
data$age <- replace(data$age, data$age >= 80, 6)
summary(data$income)
table(data$income)
data$income_bi[data$income < 7] <- 0
data$income_bi[data$income >= 7] <- 1
table(data$income_bi)    # 605 obs. with 0, 695 obs. with 1; balanced representation of levels
sapply(data, class)
data[] <- lapply(data, factor)  # make all variables as factors
sapply(data, class)
View(data)
data$income <- NULL  # removing the multilevel factor income variable
names(data)[names(data) == 'income_bi'] <- 'income'
View(data)
x <- data[,1:22]
y <- data[,23]
plot(y)
featurePlot(x=x, y=y, plot="ellipse")
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
control <- trainControl(method="cv", number=10)
set.seed(371)
fit.lda <- train(income~., data=data, method="lda", metric=metric, trControl=control)
# CART
set.seed(371)
fit.cart <- train(income~., data=data, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(371)
fit.knn <- train(income~., data=data, method="knn", metric=metric, trControl=control)
# SVM
set.seed(371)
fit.svm <- train(income~., data=data, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(371)
fit.rf <- train(income~., data=data, method="rf", metric=metric, trControl=control)
set.seed(371)
fit.lda <- train(income~., data=data, method="lda", metric=metric, trControl=control)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
dotplot(results)
predictions <- predict(fit.lda, test)
require(caTools)
set.seed(371)
split <- sample.split(data$income, SplitRatio = 0.75)
train <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
set.seed(371)
fit.lda <- train(income~., data=train, method="lda", metric=metric, trControl=control)
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
dotplot(results)
predictions <- predict(fit.lda, test)
confusionMatrix(predictions, test$income)
predictions <- predict(fit.cart, test)
confusionMatrix(predictions, test$income)
predictions <- predict(fit.rf, test)
confusionMatrix(predictions, test$income)
predictions <- predict(fit.knn, test)
confusionMatrix(predictions, test$income)
predictions <- predict(fit.svm, test)
confusionMatrix(predictions, test$income)
set.seed(371)
fit.rf <- train(income~., data=train, method="rf", metric=metric, trControl=control)
predictions <- predict(fit.rf, test)
confusionMatrix(predictions, test$income)
findCorrelation(data, cutoff = 0.9, verbose = FALSE, names = FALSE)
findCorrelation(data, cutoff = 0.9, verbose = FALSE, names = FALSE)
